<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Background · FittingObjectiveFunctions</title><meta name="title" content="Background · FittingObjectiveFunctions"/><meta property="og:title" content="Background · FittingObjectiveFunctions"/><meta property="twitter:title" content="Background · FittingObjectiveFunctions"/><meta name="description" content="Documentation for FittingObjectiveFunctions."/><meta property="og:description" content="Documentation for FittingObjectiveFunctions."/><meta property="twitter:description" content="Documentation for FittingObjectiveFunctions."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="FittingObjectiveFunctions logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">FittingObjectiveFunctions</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">FittingObjectiveFunctions</a></li><li><a class="tocitem" href="../fitting_data/">FittingData and ModelFunctions</a></li><li><span class="tocitem">Least squares objective</span><ul><li><a class="tocitem" href="../lsq_background/">Background</a></li><li><a class="tocitem" href="../lsq_implementation/">How to implement</a></li></ul></li><li><span class="tocitem">Posterior probability</span><ul><li><a class="tocitem" href="../posterior_background/">Background</a></li><li><a class="tocitem" href="../posterior_implementation/">How to implement</a></li></ul></li><li><span class="tocitem">Logarithmic posterior probability</span><ul><li class="is-active"><a class="tocitem" href>Background</a><ul class="internal"><li><a class="tocitem" href="#Product-of-small-numbers"><span>Product of small numbers</span></a></li><li><a class="tocitem" href="#Logarithmic-scale"><span>Logarithmic scale</span></a></li><li><a class="tocitem" href="#Effect-of-Logarithmic-scale"><span>Effect of Logarithmic scale</span></a></li><li><a class="tocitem" href="#Logarithmic-density-example"><span>Logarithmic density example</span></a></li></ul></li><li><a class="tocitem" href="../log_posterior_implementation/">How to implement</a></li></ul></li><li><a class="tocitem" href="../API/">API</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Logarithmic posterior probability</a></li><li class="is-active"><a href>Background</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Background</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/AntibodyPackages/FittingObjectiveFunctions" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/AntibodyPackages/FittingObjectiveFunctions/blob/main/docs/src/log_posterior_background.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Background:-Logarithmic-posterior-probability"><a class="docs-heading-anchor" href="#Background:-Logarithmic-posterior-probability">Background: Logarithmic posterior probability</a><a id="Background:-Logarithmic-posterior-probability-1"></a><a class="docs-heading-anchor-permalink" href="#Background:-Logarithmic-posterior-probability" title="Permalink"></a></h1><p>The general setting is the same as in <a href="../posterior_background/#Background:-Posterior-probability">Background:-Posterior-probability</a>. The starting point is</p><p class="math-container">\[p(\lambda \mid \{x_i\}_{i=1}^N, \{y_i\}_{i=1}^N, m) \propto  p_0(\lambda\mid \{x_i\}_{i=1}^N, m) \prod_{i_1}^n \ell_i(y_i\mid \lambda, x_i,m)\ ,\]</p><p>assuming the likelihoods <span>$\ell_i$</span> are known.</p><h2 id="Product-of-small-numbers"><a class="docs-heading-anchor" href="#Product-of-small-numbers">Product of small numbers</a><a id="Product-of-small-numbers-1"></a><a class="docs-heading-anchor-permalink" href="#Product-of-small-numbers" title="Permalink"></a></h2><p>In the formula for the posterior likelihood, it can happen that many small numbers (close to zero) need to be multiplied together. Because <a href="https://en.wikipedia.org/wiki/Floating-point_arithmetic#Representable_numbers,_conversion_and_rounding">floating point numbers</a> can only represent numbers up to a certain precision, such products, though theoretically non-zero, tend to be rounded to zero.</p><p>For example, consider the following array as the likelihood values:</p><pre><code class="language-julia hljs">using Distributions, BenchmarkTools
small_values = [pdf(Normal(0,1),10+i) for i in 1:10]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">10-element Vector{Float64}:
 2.1188192535093538e-27
 2.1463837356630605e-32
 7.998827757006813e-38
 1.0966065593889713e-43
 5.530709549844416e-50
 1.0261630727919036e-56
 7.004182134318583e-64
 1.758749542595104e-71
 1.624636036773608e-79
 5.520948362159764e-88</code></pre><p>Although the values are non-zero, the product is rounded to zero:</p><pre><code class="language-julia hljs">prod(small_values)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">0.0</code></pre><p>One could use floating point types with higher precision:</p><pre><code class="language-julia hljs">small_values_high_precision = BigFloat.(small_values)
prod(small_values_high_precision)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2.501536784247610370239770054628823917126174421661076782600307193509415451541111e-544</code></pre><p>However, this entails a huge performance loss together with increased memory usage: </p><pre><code class="language-julia hljs">@benchmark prod(small_values)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">BenchmarkTools.Trial: 10000 samples with 999 evaluations.
 Range <span class="sgr90">(</span><span class="sgr36"><span class="sgr1">min</span></span> … <span class="sgr35">max</span><span class="sgr90">):  </span><span class="sgr36"><span class="sgr1">11.076 ns</span></span> … <span class="sgr35"> 2.129 μs</span>  <span class="sgr90">┊</span> GC <span class="sgr90">(</span>min … max<span class="sgr90">): </span>0.00% … 98.51%
 Time  <span class="sgr90">(</span><span class="sgr34"><span class="sgr1">median</span></span><span class="sgr90">):     </span><span class="sgr34"><span class="sgr1">12.184 ns              </span></span><span class="sgr90">┊</span> GC <span class="sgr90">(</span>median<span class="sgr90">):    </span>0.00%
 Time  <span class="sgr90">(</span><span class="sgr32"><span class="sgr1">mean</span></span> ± <span class="sgr32">σ</span><span class="sgr90">):   </span><span class="sgr32"><span class="sgr1">13.177 ns</span></span> ± <span class="sgr32">26.928 ns</span>  <span class="sgr90">┊</span> GC <span class="sgr90">(</span>mean ± σ<span class="sgr90">):  </span>3.07% ±  1.66%

     ▁▇██▅▆▅<span class="sgr34">▇</span>▅▂      <span class="sgr32"> </span>                                         
  ▁▂▄███████<span class="sgr34">█</span>███▅▄▃▃▂<span class="sgr32">▂</span>▂▂▂▁▁▁▁▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁ ▃
  11.1 ns<span class="sgr90">         Histogram: frequency by time</span>        17.7 ns <span class="sgr1">&lt;</span>

 Memory estimate<span class="sgr90">: </span><span class="sgr33">16 bytes</span>, allocs estimate<span class="sgr90">: </span><span class="sgr33">1</span>.</code></pre><pre><code class="language-julia hljs">@benchmark prod(small_values_high_precision)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">BenchmarkTools.Trial: 10000 samples with 250 evaluations.
 Range <span class="sgr90">(</span><span class="sgr36"><span class="sgr1">min</span></span> … <span class="sgr35">max</span><span class="sgr90">):  </span><span class="sgr36"><span class="sgr1">302.916 ns</span></span> … <span class="sgr35">  6.212 μs</span>  <span class="sgr90">┊</span> GC <span class="sgr90">(</span>min … max<span class="sgr90">): </span>0.00% … 92.86%
 Time  <span class="sgr90">(</span><span class="sgr34"><span class="sgr1">median</span></span><span class="sgr90">):     </span><span class="sgr34"><span class="sgr1">325.644 ns               </span></span><span class="sgr90">┊</span> GC <span class="sgr90">(</span>median<span class="sgr90">):    </span>0.00%
 Time  <span class="sgr90">(</span><span class="sgr32"><span class="sgr1">mean</span></span> ± <span class="sgr32">σ</span><span class="sgr90">):   </span><span class="sgr32"><span class="sgr1">355.333 ns</span></span> ± <span class="sgr32">318.337 ns</span>  <span class="sgr90">┊</span> GC <span class="sgr90">(</span>mean ± σ<span class="sgr90">):  </span>6.23% ±  6.50%

    ▂   ▅█<span class="sgr34">▃</span>         <span class="sgr32"> </span>                                            
  ▃▇██▅▆██<span class="sgr34">█</span>▇▅▅▄▄▅▅▆▅<span class="sgr32">▄</span>▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂ ▃
  303 ns<span class="sgr90">           Histogram: frequency by time</span>          479 ns <span class="sgr1">&lt;</span>

 Memory estimate<span class="sgr90">: </span><span class="sgr33">936 bytes</span>, allocs estimate<span class="sgr90">: </span><span class="sgr33">18</span>.</code></pre><h2 id="Logarithmic-scale"><a class="docs-heading-anchor" href="#Logarithmic-scale">Logarithmic scale</a><a id="Logarithmic-scale-1"></a><a class="docs-heading-anchor-permalink" href="#Logarithmic-scale" title="Permalink"></a></h2><p>Since posterior probabilities are often unnormalized anyways, one is not interested in the particular values, but only in relative differences. But then, any strictly monotonic function can be applied to compare relative differences. A convenient choice for such a strictly monotonic (increasing) function is the logarithm.</p><p>Note that because of proportionality, there is a constant <span>$\alpha &gt; 0$</span> such that</p><p class="math-container">\[p(\lambda \mid \{x_i\}_{i=1}^N, \{y_i\}_{i=1}^N, m) =  \alpha \cdot  p_0(\lambda\mid \{x_i\}_{i=1}^N, m) \prod_{i_1}^n \ell_i(y_i\mid \lambda, x_i,m)\ .\]</p><p>Applying the natural logarithm leads to:</p><p class="math-container">\[\begin{aligned}
\ln (p(\lambda \mid \{x_i\}_{i=1}^N, \{y_i\}_{i=1}^N, m)) &amp;=  \ln \left(\alpha \cdot  p_0(\lambda\mid \{x_i\}_{i=1}^N, m) \prod_{i_1}^n \ell_i(y_i\mid \lambda, x_i,m) \right) \\ 
&amp;= \ln(p_0(\lambda\mid \{x_i\}_{i=1}^N, m)) + \sum_{i=1}^N \ln(\ell_i(y_i\mid \lambda, x_i,m)) + \ln(\alpha)
\end{aligned}\]</p><p>Using the logarithm allowed to exchange the multiplication of small numbers for an addition in the logarithmic scale, at the cost of having to calculate the logarithm of every value. However, the cost of calculating the logarithm is the worst case scenario. In many cases, it is possible if not easier to implement logarithms of the involved densities (e.g. for the normal distribution, laplace distribution, etc.).</p><p>To shorten the notation, denote the logarithms of the distributions by <span>$L_p = \ln \circ\ p$</span> for the posterior, <span>$L_i= \ln \circ\ \ell_i$</span> for the likelihoods and <span>$L_0 =\ln \circ\ p_0$</span> for the prior: </p><p class="math-container">\[L_p(\lambda \mid \{x_i\}_{i=1}^N, \{y_i\}_{i=1}^N, m) =   L_0(\lambda\mid \{x_i\}_{i=1}^N, m) +  \sum_{i_1}^n L_i(y_i\mid \lambda, x_i,m) + \text{const.}\]</p><h2 id="Effect-of-Logarithmic-scale"><a class="docs-heading-anchor" href="#Effect-of-Logarithmic-scale">Effect of Logarithmic scale</a><a id="Effect-of-Logarithmic-scale-1"></a><a class="docs-heading-anchor-permalink" href="#Effect-of-Logarithmic-scale" title="Permalink"></a></h2><p>Using the logarithm has two effects. First of all, the product becomes a sum. This alone would suffice to prevent the rounding to zero problem:</p><pre><code class="language-julia hljs">sum(small_values)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2.1188407174266987e-27</code></pre><p>In addition, the logarithm has the effect of compressing the number scale for numbers larger than 1 and to stretch out the number scale for numbers between 0 and 1:</p><pre><code class="language-julia hljs">log_values = log.(small_values)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">10-element Vector{Float64}:
  -61.418938533204674
  -72.91893853320467
  -85.41893853320467
  -98.91893853320467
 -113.41893853320467
 -128.91893853320468
 -145.41893853320468
 -162.91893853320468
 -181.41893853320468
 -200.91893853320468</code></pre><p>Of course, the sum is still non-zero:</p><pre><code class="language-julia hljs">sum(log_values)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">-1251.6893853320469</code></pre><p>While the use of higher precision floating point numbers (<code>BigFloat</code>) meant a huge performance loss, the log scale method dose not impair performance:</p><pre><code class="language-julia hljs">@benchmark sum(log_values)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">BenchmarkTools.Trial: 10000 samples with 999 evaluations.
 Range <span class="sgr90">(</span><span class="sgr36"><span class="sgr1">min</span></span> … <span class="sgr35">max</span><span class="sgr90">):  </span><span class="sgr36"><span class="sgr1">11.772 ns</span></span> … <span class="sgr35">778.593 ns</span>  <span class="sgr90">┊</span> GC <span class="sgr90">(</span>min … max<span class="sgr90">): </span>0.00% … 95.54%
 Time  <span class="sgr90">(</span><span class="sgr34"><span class="sgr1">median</span></span><span class="sgr90">):     </span><span class="sgr34"><span class="sgr1">14.165 ns               </span></span><span class="sgr90">┊</span> GC <span class="sgr90">(</span>median<span class="sgr90">):    </span>0.00%
 Time  <span class="sgr90">(</span><span class="sgr32"><span class="sgr1">mean</span></span> ± <span class="sgr32">σ</span><span class="sgr90">):   </span><span class="sgr32"><span class="sgr1">14.862 ns</span></span> ± <span class="sgr32"> 10.557 ns</span>  <span class="sgr90">┊</span> GC <span class="sgr90">(</span>mean ± σ<span class="sgr90">):  </span>1.13% ±  1.64%

            ▃▁ ▄█▇<span class="sgr34">▅</span>▂ ▁▄<span class="sgr32">▂</span>                                        
  ▁▁▁▂▂▃▃▄▆███████<span class="sgr34">█</span>████<span class="sgr32">█</span>▇▅▄▃▃▃▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁ ▃
  11.8 ns<span class="sgr90">         Histogram: frequency by time</span>         20.5 ns <span class="sgr1">&lt;</span>

 Memory estimate<span class="sgr90">: </span><span class="sgr33">16 bytes</span>, allocs estimate<span class="sgr90">: </span><span class="sgr33">1</span>.</code></pre><h2 id="Logarithmic-density-example"><a class="docs-heading-anchor" href="#Logarithmic-density-example">Logarithmic density example</a><a id="Logarithmic-density-example-1"></a><a class="docs-heading-anchor-permalink" href="#Logarithmic-density-example" title="Permalink"></a></h2><p>In <a href="#Logarithmic-scale">Logarithmic-scale</a> it was mentioned that some distributions are even easier to be implemented in a logarithmic scale. This is not only the case for the definition of densities from scratch, but also applies for <code>Distributions.jl</code>. Observe that &quot;far&quot; away from the mean, the pdf of a normal distribution is rounded to zero:</p><pre><code class="language-julia hljs">pdf(Normal(0,1),100)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">0.0</code></pre><p>Obviously, the logarithm cannot be applied to this. However, <code>Distributions.jl</code> offers a <code>logpdf</code> function:</p><pre><code class="language-julia hljs">logpdf(Normal(0,1),100)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">-5000.918938533205</code></pre><p>This allows values even further away from the mean:</p><pre><code class="language-julia hljs">logpdf(Normal(0,1),10^20)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">-3.0157549656954986e37</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../posterior_implementation/">« How to implement</a><a class="docs-footer-nextpage" href="../log_posterior_implementation/">How to implement »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.3.0 on <span class="colophon-date" title="Wednesday 10 July 2024 23:10">Wednesday 10 July 2024</span>. Using Julia version 1.10.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
