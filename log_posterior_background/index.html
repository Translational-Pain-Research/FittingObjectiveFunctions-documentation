<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Background · FittingObjectiveFunctions.jl</title><meta name="title" content="Background · FittingObjectiveFunctions.jl"/><meta property="og:title" content="Background · FittingObjectiveFunctions.jl"/><meta property="twitter:title" content="Background · FittingObjectiveFunctions.jl"/><meta name="description" content="Documentation for FittingObjectiveFunctions.jl."/><meta property="og:description" content="Documentation for FittingObjectiveFunctions.jl."/><meta property="twitter:description" content="Documentation for FittingObjectiveFunctions.jl."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="FittingObjectiveFunctions.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">FittingObjectiveFunctions.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">FittingObjectiveFunctions</a></li><li><a class="tocitem" href="../fitting_data/">FittingData and ModelFunctions</a></li><li><span class="tocitem">Least squares objective</span><ul><li><a class="tocitem" href="../lsq_background/">Background</a></li><li><a class="tocitem" href="../lsq_implementation/">How to implement</a></li></ul></li><li><span class="tocitem">Posterior probability</span><ul><li><a class="tocitem" href="../posterior_background/">Background</a></li><li><a class="tocitem" href="../posterior_implementation/">How to implement</a></li></ul></li><li><span class="tocitem">Logarithmic posterior probability</span><ul><li class="is-active"><a class="tocitem" href>Background</a><ul class="internal"><li><a class="tocitem" href="#Product-of-small-numbers"><span>Product of small numbers</span></a></li><li><a class="tocitem" href="#Logarithmic-scale"><span>Logarithmic scale</span></a></li><li><a class="tocitem" href="#Effect-of-the-logarithmic-scale"><span>Effect of the logarithmic scale</span></a></li><li><a class="tocitem" href="#Logarithmic-densities-and-Distributions.jl"><span>Logarithmic densities and <code>Distributions.jl</code></span></a></li></ul></li><li><a class="tocitem" href="../log_posterior_implementation/">How to implement</a></li></ul></li><li><a class="tocitem" href="../API/">API</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Logarithmic posterior probability</a></li><li class="is-active"><a href>Background</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Background</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/Translational-Pain-Research/FittingObjectiveFunctions.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/Translational-Pain-Research/FittingObjectiveFunctions.jl/blob/main/docs/src/log_posterior_background.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Background:-Logarithmic-posterior-probability"><a class="docs-heading-anchor" href="#Background:-Logarithmic-posterior-probability">Background: Logarithmic posterior probability</a><a id="Background:-Logarithmic-posterior-probability-1"></a><a class="docs-heading-anchor-permalink" href="#Background:-Logarithmic-posterior-probability" title="Permalink"></a></h1><p>The general setting is the same as in <a href="../posterior_background/#Background:-Posterior-probability">Background: Posterior probability</a>. The starting point is</p><p class="math-container">\[p(\lambda \mid \{x_i\}_{i=1}^N, \{y_i\}_{i=1}^N, m) \propto  p_0(\lambda\mid \{x_i\}_{i=1}^N, m) \prod_{i=1}^n \ell_i(y_i\mid \lambda, x_i,m)\ ,\]</p><p>assuming the likelihoods <span>$\ell_i$</span> are known.</p><h2 id="Product-of-small-numbers"><a class="docs-heading-anchor" href="#Product-of-small-numbers">Product of small numbers</a><a id="Product-of-small-numbers-1"></a><a class="docs-heading-anchor-permalink" href="#Product-of-small-numbers" title="Permalink"></a></h2><p>In the formula for the posterior likelihood, it can happen that many small numbers (close to zero) need to be multiplied together. Because <a href="https://en.wikipedia.org/wiki/Floating-point_arithmetic#Representable_numbers,_conversion_and_rounding">floating point numbers</a> can only represent numbers up to a certain precision, such products, though theoretically non-zero, tend to be rounded to zero.</p><p>For example, consider the following array as the likelihood values:</p><pre><code class="language-julia hljs">using Distributions, BenchmarkTools
small_values = [pdf(Normal(0,1),10+i) for i in 1:10]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">10-element Vector{Float64}:
 2.1188192535093538e-27
 2.1463837356630605e-32
 7.998827757006813e-38
 1.0966065593889713e-43
 5.530709549844416e-50
 1.0261630727919036e-56
 7.004182134318583e-64
 1.758749542595104e-71
 1.624636036773608e-79
 5.520948362159764e-88</code></pre><p>Although the values are non-zero, the product is rounded to zero:</p><pre><code class="language-julia hljs">prod(small_values)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">0.0</code></pre><p>One could use floating point types with higher precision:</p><pre><code class="language-julia hljs">small_values_high_precision = BigFloat.(small_values)
prod(small_values_high_precision)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2.501536784247610370239770054628823917126174421661076782600307193509415451541111e-544</code></pre><p>However, this means a huge performance loss together with increased memory usage: </p><pre><code class="language-julia hljs">@benchmark prod(small_values)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">BenchmarkTools.Trial: 10000 samples with 999 evaluations.
 Range <span class="sgr90">(</span><span class="sgr36"><span class="sgr1">min</span></span> … <span class="sgr35">max</span><span class="sgr90">):  </span><span class="sgr36"><span class="sgr1">11.148 ns</span></span> … <span class="sgr35"> 1.760 μs</span>  <span class="sgr90">┊</span> GC <span class="sgr90">(</span>min … max<span class="sgr90">): </span>0.00% … 97.37%
 Time  <span class="sgr90">(</span><span class="sgr34"><span class="sgr1">median</span></span><span class="sgr90">):     </span><span class="sgr34"><span class="sgr1">12.811 ns              </span></span><span class="sgr90">┊</span> GC <span class="sgr90">(</span>median<span class="sgr90">):    </span>0.00%
 Time  <span class="sgr90">(</span><span class="sgr32"><span class="sgr1">mean</span></span> ± <span class="sgr32">σ</span><span class="sgr90">):   </span><span class="sgr32"><span class="sgr1">14.342 ns</span></span> ± <span class="sgr32">24.389 ns</span>  <span class="sgr90">┊</span> GC <span class="sgr90">(</span>mean ± σ<span class="sgr90">):  </span>2.58% ±  1.66%

     ▃▃██▅<span class="sgr34">█</span>       <span class="sgr32"> </span>                                            
  ▁▂▄█████<span class="sgr34">█</span>█▆▃▂▂▂▁<span class="sgr32">▁</span>▁▁▂▂▂▃▄▅▄▃▄▄▄▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁ ▂
  11.1 ns<span class="sgr90">         Histogram: frequency by time</span>          23 ns <span class="sgr1">&lt;</span>

 Memory estimate<span class="sgr90">: </span><span class="sgr33">16 bytes</span>, allocs estimate<span class="sgr90">: </span><span class="sgr33">1</span>.</code></pre><pre><code class="language-julia hljs">@benchmark prod(small_values_high_precision)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">BenchmarkTools.Trial: 10000 samples with 246 evaluations.
 Range <span class="sgr90">(</span><span class="sgr36"><span class="sgr1">min</span></span> … <span class="sgr35">max</span><span class="sgr90">):  </span><span class="sgr36"><span class="sgr1">307.423 ns</span></span> … <span class="sgr35">  5.995 μs</span>  <span class="sgr90">┊</span> GC <span class="sgr90">(</span>min … max<span class="sgr90">): </span>0.00% … 91.68%
 Time  <span class="sgr90">(</span><span class="sgr34"><span class="sgr1">median</span></span><span class="sgr90">):     </span><span class="sgr34"><span class="sgr1">346.510 ns               </span></span><span class="sgr90">┊</span> GC <span class="sgr90">(</span>median<span class="sgr90">):    </span>0.00%
 Time  <span class="sgr90">(</span><span class="sgr32"><span class="sgr1">mean</span></span> ± <span class="sgr32">σ</span><span class="sgr90">):   </span><span class="sgr32"><span class="sgr1">369.997 ns</span></span> ± <span class="sgr32">263.214 ns</span>  <span class="sgr90">┊</span> GC <span class="sgr90">(</span>mean ± σ<span class="sgr90">):  </span>4.77% ±  6.22%

   ▁  █▃ <span class="sgr34"> </span>▄▆  <span class="sgr32"> </span>                                                  
  ▃██▇██▇<span class="sgr34">▆</span>███▅<span class="sgr32">▄</span>▄▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▂▂▁▂ ▃
  307 ns<span class="sgr90">           Histogram: frequency by time</span>          629 ns <span class="sgr1">&lt;</span>

 Memory estimate<span class="sgr90">: </span><span class="sgr33">936 bytes</span>, allocs estimate<span class="sgr90">: </span><span class="sgr33">18</span>.</code></pre><h2 id="Logarithmic-scale"><a class="docs-heading-anchor" href="#Logarithmic-scale">Logarithmic scale</a><a id="Logarithmic-scale-1"></a><a class="docs-heading-anchor-permalink" href="#Logarithmic-scale" title="Permalink"></a></h2><p>Since posterior probabilities are often unnormalized, one is not interested in the particular values, but only in relative differences. But then, any strictly monotonically increasing function can be applied to compare relative differences. A convenient choice for such a strictly monotonically increasing function is the  natural logarithm.</p><p>Note that because of proportionality, there is a constant <span>$\alpha &gt; 0$</span> such that</p><p class="math-container">\[p(\lambda \mid \{x_i\}_{i=1}^N, \{y_i\}_{i=1}^N, m) =  \alpha \cdot  p_0(\lambda\mid \{x_i\}_{i=1}^N, m) \prod_{i=1}^n \ell_i(y_i\mid \lambda, x_i,m)\ .\]</p><p>Applying the natural logarithm leads to:</p><p class="math-container">\[\begin{aligned}
\ln (p(\lambda \mid \{x_i\}_{i=1}^N, \{y_i\}_{i=1}^N, m)) &amp;=  \ln \left(\alpha \cdot  p_0(\lambda\mid \{x_i\}_{i=1}^N, m) \prod_{i=1}^n \ell_i(y_i\mid \lambda, x_i,m) \right) \\ 
&amp;= \ln(p_0(\lambda\mid \{x_i\}_{i=1}^N, m)) + \sum_{i=1}^N \ln(\ell_i(y_i\mid \lambda, x_i,m)) + \ln(\alpha)
\end{aligned}\]</p><p>Using the logarithm allowed to exchange the multiplication of small numbers for an addition in the logarithmic scale, at the cost of having to calculate the logarithm of every value. However, the cost of calculating the logarithm is the worst case scenario. In many cases, it is possible if not easier to implement the logarithm of the involved densities (e.g. for the normal distribution, Laplace distribution, etc.).</p><p>To shorten the notation, we write <span>$L_p = \ln \circ\ p$</span> for the posterior, <span>$L_i= \ln \circ\ \ell_i$</span> for the likelihoods and <span>$L_0 =\ln \circ\ p_0$</span> for the prior. Then the log-posterior reads </p><p class="math-container">\[L_p(\lambda \mid \{x_i\}_{i=1}^N, \{y_i\}_{i=1}^N, m) =   L_0(\lambda\mid \{x_i\}_{i=1}^N, m) +  \sum_{i_1}^n L_i(y_i\mid \lambda, x_i,m) + \text{const.}\]</p><h2 id="Effect-of-the-logarithmic-scale"><a class="docs-heading-anchor" href="#Effect-of-the-logarithmic-scale">Effect of the logarithmic scale</a><a id="Effect-of-the-logarithmic-scale-1"></a><a class="docs-heading-anchor-permalink" href="#Effect-of-the-logarithmic-scale" title="Permalink"></a></h2><p>Using the logarithm has two effects. First of all, the product becomes a sum. This alone would suffice to prevent the rounding to zero problem:</p><pre><code class="language-julia hljs">sum(small_values)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2.1188407174266987e-27</code></pre><p>In addition, the logarithm has the effect of compressing the number scale for numbers larger than 1 and to stretch out the number scale for numbers between 0 and 1:</p><pre><code class="language-julia hljs">log_values = log.(small_values)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">10-element Vector{Float64}:
  -61.418938533204674
  -72.91893853320467
  -85.41893853320467
  -98.91893853320467
 -113.41893853320467
 -128.91893853320468
 -145.41893853320468
 -162.91893853320468
 -181.41893853320468
 -200.91893853320468</code></pre><p>Of course, the sum is still non-zero:</p><pre><code class="language-julia hljs">sum(log_values)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">-1251.6893853320469</code></pre><p>While the use of higher precision floating point numbers (<code>BigFloat</code>) lead to a huge performance loss, the log scale method dose not impair performance:</p><pre><code class="language-julia hljs">@benchmark sum(log_values)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">BenchmarkTools.Trial: 10000 samples with 999 evaluations.
 Range <span class="sgr90">(</span><span class="sgr36"><span class="sgr1">min</span></span> … <span class="sgr35">max</span><span class="sgr90">):  </span><span class="sgr36"><span class="sgr1">12.566 ns</span></span> … <span class="sgr35">628.090 ns</span>  <span class="sgr90">┊</span> GC <span class="sgr90">(</span>min … max<span class="sgr90">): </span>0.00% … 91.52%
 Time  <span class="sgr90">(</span><span class="sgr34"><span class="sgr1">median</span></span><span class="sgr90">):     </span><span class="sgr34"><span class="sgr1">14.248 ns               </span></span><span class="sgr90">┊</span> GC <span class="sgr90">(</span>median<span class="sgr90">):    </span>0.00%
 Time  <span class="sgr90">(</span><span class="sgr32"><span class="sgr1">mean</span></span> ± <span class="sgr32">σ</span><span class="sgr90">):   </span><span class="sgr32"><span class="sgr1">15.603 ns</span></span> ± <span class="sgr32">  9.610 ns</span>  <span class="sgr90">┊</span> GC <span class="sgr90">(</span>mean ± σ<span class="sgr90">):  </span>0.89% ±  1.57%

   ▃▅▆███<span class="sgr34">▇</span>▆▅▅▄<span class="sgr32">▃</span>▂▁ ▂▂▁▃▄▃▂▂▂▂▁▁ ▁▁ ▁  ▁▁▃▂                      ▂
  ███████<span class="sgr34">█</span>████<span class="sgr32">█</span>██▇█████████████████▇███████▇▇█▆▇▆▆▆▆▅▅▆▅▆▄▆▄▅▅ █
  12.6 ns<span class="sgr90">       Histogram: <span class="sgr1">log(</span>frequency<span class="sgr1">)</span> by time</span>      27.6 ns <span class="sgr1">&lt;</span>

 Memory estimate<span class="sgr90">: </span><span class="sgr33">16 bytes</span>, allocs estimate<span class="sgr90">: </span><span class="sgr33">1</span>.</code></pre><h2 id="Logarithmic-densities-and-Distributions.jl"><a class="docs-heading-anchor" href="#Logarithmic-densities-and-Distributions.jl">Logarithmic densities and <code>Distributions.jl</code></a><a id="Logarithmic-densities-and-Distributions.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Logarithmic-densities-and-Distributions.jl" title="Permalink"></a></h2><p>As mentioned in  <a href="#Logarithmic-scale">Logarithmic scale</a> it can be easier to implement some density functions in a logarithmic scale. This is not only true for the implementations from scratch, but also for <a href="https://juliastats.org/Distributions.jl/stable/"><code>Distributions.jl</code></a>. Observe that &quot;far&quot; away from the mean, the pdf of a normal distribution is rounded to zero:</p><pre><code class="language-julia hljs">pdf(Normal(0,1),100)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">0.0</code></pre><p>Obviously, the logarithm cannot be applied to this. However, <code>Distributions.jl</code> offers a <code>logpdf</code> function:</p><pre><code class="language-julia hljs">logpdf(Normal(0,1),100)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">-5000.918938533205</code></pre><p>This allows values even further away from the mean:</p><pre><code class="language-julia hljs">logpdf(Normal(0,1),10^20)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">-3.0157549656954986e37</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../posterior_implementation/">« How to implement</a><a class="docs-footer-nextpage" href="../log_posterior_implementation/">How to implement »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.7.0 on <span class="colophon-date" title="Thursday 12 September 2024 18:11">Thursday 12 September 2024</span>. Using Julia version 1.10.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
